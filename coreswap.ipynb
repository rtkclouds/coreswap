{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aula1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZlEIEW_REmg",
        "colab_type": "text"
      },
      "source": [
        "## Exercicios - Troca de faces por reposicionamento da face\n",
        "Neste exercicio iremos inverter os rostos de duas imagens por meio da detecção\n",
        "da posiçaõ da face.\n",
        "\n",
        "\n",
        "Iremos utilizar pare esses exercicios:\n",
        "\n",
        "\n",
        "1) shape_predictor_68_face_landmarks.dat - Detecta os 68 principais pontos de referencia da face \n",
        "\n",
        "\n",
        "2) ffmpeg - conversor video/imagem\n",
        "\n",
        "3) python - codigo do programa\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "![alt text](https://www.piacabucunews.com.br/wp-content/uploads/2017/06/2eebf0ae-85b5-4529-a556-72d2a50c2301-48.jpg)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Com esse exercicio você será capaz de fazer 2 operações:\n",
        "\n",
        "1) Inverter rosto em imagens;\n",
        "\n",
        "2) Iverter rosto em videos;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxd9XzjLVrkf",
        "colab_type": "text"
      },
      "source": [
        "# Instalar programas\n",
        "1) install ffmpeg\n",
        "2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aF1lRA6Vsok",
        "colab_type": "code",
        "outputId": "029a5572-655c-4041-a520-b5a047ce8741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!apt install ffmpeg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxNfTag2RA1M",
        "colab_type": "code",
        "outputId": "410ba436-e4bf-4d21-e09b-87868aae06de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!wget https://github.com/AKSHAYUBHAT/TensorFace/blob/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat?raw=true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-11 01:32:23--  https://github.com/AKSHAYUBHAT/TensorFace/blob/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/AKSHAYUBHAT/TensorFace/raw/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat [following]\n",
            "--2020-05-11 01:32:24--  https://github.com/AKSHAYUBHAT/TensorFace/raw/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/AKSHAYUBHAT/TensorFace/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat [following]\n",
            "--2020-05-11 01:32:24--  https://raw.githubusercontent.com/AKSHAYUBHAT/TensorFace/master/openface/models/dlib/shape_predictor_68_face_landmarks.dat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99693937 (95M) [application/octet-stream]\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat?raw=true.1’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  95.08M  82.9MB/s    in 1.1s    \n",
            "\n",
            "2020-05-11 01:32:25 (82.9 MB/s) - ‘shape_predictor_68_face_landmarks.dat?raw=true.1’ saved [99693937/99693937]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4chfpztjihU",
        "colab_type": "text"
      },
      "source": [
        "# EXERCICIO 1 - Troca de rosto em imagem\n",
        "\n",
        "-Substitua os nomes \"IMAGE1\" e \"IMAGE2\" pelo nome das imagens que você deseja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdfDOyPRRERb",
        "colab_type": "code",
        "outputId": "1a26afa1-1357-4c2b-ec25-9d8f6a6e333d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy\n",
        "\n",
        "import sys\n",
        "IMAGE1='./sa.jpeg'\n",
        "IMAGE2='./b.jpg'\n",
        "PREDICTOR_PATH = \"shape_predictor_68_face_landmarks.dat?raw=true\"\n",
        "SCALE_FACTOR = 1\n",
        "FEATHER_AMOUNT = 9\n",
        "\n",
        "FACE_POINTS = list(range(17, 68))\n",
        "MOUTH_POINTS = list(range(48, 61))\n",
        "RIGHT_BROW_POINTS = list(range(17, 22))\n",
        "LEFT_BROW_POINTS = list(range(22, 27))\n",
        "RIGHT_EYE_POINTS = list(range(36, 42))\n",
        "LEFT_EYE_POINTS = list(range(42, 48))\n",
        "NOSE_POINTS = list(range(27, 35))\n",
        "JAW_POINTS = list(range(0, 17))\n",
        "\n",
        "# Pontos usados para alinhar as imagens.\n",
        "ALIGN_POINTS = (LEFT_BROW_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS +\n",
        "                               RIGHT_BROW_POINTS + NOSE_POINTS + MOUTH_POINTS+JAW_POINTS)\n",
        "\n",
        "# Aponta da segunda imagem para sobrepor na primeira. O casco convexo de cada O elemento \n",
        "# será sobreposto.\n",
        "OVERLAY_POINTS = [\n",
        "    LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,\n",
        "    NOSE_POINTS + MOUTH_POINTS,JAW_POINTS\n",
        "]\n",
        "\n",
        "# Quantidade de desfoque a ser usada durante a correção de cores, como uma fração da\n",
        "# distância pupilar.\n",
        "COLOUR_CORRECT_BLUR_FRAC = .8\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(PREDICTOR_PATH)\n",
        "\n",
        "class TooManyFaces(Exception):\n",
        "    pass\n",
        "\n",
        "class NoFaces(Exception):\n",
        "    pass\n",
        "\n",
        "def get_landmarks(im):\n",
        "    rects = detector(im, 1)\n",
        "    \n",
        "    if len(rects) > 1:\n",
        "        raise TooManyFaces\n",
        "    if len(rects) == 0:\n",
        "        raise NoFaces\n",
        "\n",
        "    return numpy.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])\n",
        "\n",
        "def annotate_landmarks(im, landmarks):\n",
        "    im = im.copy()\n",
        "    for idx, point in enumerate(landmarks):\n",
        "        pos = (point[0, 0], point[0, 1])\n",
        "        cv2.putText(im, str(idx), pos,\n",
        "                    fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n",
        "                    fontScale=0.4,\n",
        "                    color=(0, 0, 255))\n",
        "        cv2.circle(im, pos, 3, color=(0, 255, 255))\n",
        "    return im\n",
        "\n",
        "def draw_convex_hull(im, points, color):\n",
        "    points = cv2.convexHull(points)\n",
        "    cv2.fillConvexPoly(im, points, color=color)\n",
        "\n",
        "def get_face_mask(im, landmarks):\n",
        "    im = numpy.zeros(im.shape[:2], dtype=numpy.float64)\n",
        "\n",
        "    for group in OVERLAY_POINTS:\n",
        "        draw_convex_hull(im,\n",
        "                         landmarks[group],\n",
        "                         color=1)\n",
        "\n",
        "    im = numpy.array([im, im, im]).transpose((1, 2, 0))\n",
        "\n",
        "    im = (cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0) > 0) * 1.0\n",
        "    im = cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0)\n",
        "\n",
        "    return im\n",
        "    \n",
        "def transformation_from_points(points1, points2):\n",
        "    \"\"\"\n",
        "    Return an affine transformation [s * R | T] such that:\n",
        "\n",
        "        sum ||s*R*p1,i + T - p2,i||^2\n",
        "\n",
        "    is minimized.\n",
        "\n",
        "    \"\"\"\n",
        "    # Resolva o problema  subtraindo os centróides, escalando pelo\n",
        "    # desvio padrão e, em seguida, usando o SVD para calcular a rotação. Vejo\n",
        "    # para mais detalhes:\n",
        "    # https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n",
        "\n",
        "    points1 = points1.astype(numpy.float64)\n",
        "    points2 = points2.astype(numpy.float64)\n",
        "\n",
        "    c1 = numpy.mean(points1, axis=0)\n",
        "    c2 = numpy.mean(points2, axis=0)\n",
        "    points1 -= c1\n",
        "    points2 -= c2\n",
        "\n",
        "    s1 = numpy.std(points1)\n",
        "    s2 = numpy.std(points2)\n",
        "    points1 /= s1\n",
        "    points2 /= s2\n",
        "\n",
        "    U, S, Vt = numpy.linalg.svd(points1.T * points2)\n",
        "\n",
        "    # OR que buscamos é, de fato, a transposição da dada por U * Vt.\n",
        "    # é porque a formulação acima assume que a matriz segue à direita\n",
        "    # (com vetores de linha) onde, como nossa solução requer que a matriz esteja na\n",
        "    # esquerda (com vetores de coluna).\n",
        "    R = (U * Vt).T\n",
        "\n",
        "    return numpy.vstack([numpy.hstack(((s2 / s1) * R,\n",
        "                                       c2.T - (s2 / s1) * R * c1.T)),\n",
        "                         numpy.matrix([0., 0., 1.])])\n",
        "\n",
        "def read_im_and_landmarks(fname):\n",
        "    im = cv2.imread(fname, cv2.IMREAD_COLOR)\n",
        "    im = cv2.resize(im, (im.shape[1] * SCALE_FACTOR,\n",
        "                         im.shape[0] * SCALE_FACTOR))\n",
        "    s = get_landmarks(im)\n",
        "\n",
        "    return im, s\n",
        "\n",
        "def warp_im(im, M, dshape):\n",
        "    output_im = numpy.zeros(dshape, dtype=im.dtype)\n",
        "    cv2.warpAffine(im,\n",
        "                   M[:2],\n",
        "                   (dshape[1], dshape[0]),\n",
        "                   dst=output_im,\n",
        "                   borderMode=cv2.BORDER_TRANSPARENT,\n",
        "                   flags=cv2.WARP_INVERSE_MAP)\n",
        "    return output_im\n",
        "\n",
        "def correct_colours(im1, im2, landmarks1):\n",
        "    blur_amount = COLOUR_CORRECT_BLUR_FRAC * numpy.linalg.norm(\n",
        "                              numpy.mean(landmarks1[LEFT_EYE_POINTS], axis=0) -\n",
        "                              numpy.mean(landmarks1[RIGHT_EYE_POINTS], axis=0))\n",
        "    blur_amount = int(blur_amount)\n",
        "    if blur_amount % 2 == 0:\n",
        "        blur_amount += 1\n",
        "    im1_blur = cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)\n",
        "    im2_blur = cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)\n",
        "\n",
        "    # Evitar divizão por zero.\n",
        "    im2_blur += (128 * (im2_blur <= 1.0)).astype(im2_blur.dtype)\n",
        "\n",
        "    return (im2.astype(numpy.float64) * im1_blur.astype(numpy.float64) /\n",
        "                                                im2_blur.astype(numpy.float64))\n",
        "\n",
        "im1, landmarks1 = read_im_and_landmarks(IMAGE1)\n",
        "im2, landmarks2 = read_im_and_landmarks(IMAGE2)\n",
        "\n",
        "M = transformation_from_points(landmarks1[ALIGN_POINTS],\n",
        "                               landmarks2[ALIGN_POINTS])\n",
        "\n",
        "mask = get_face_mask(im2, landmarks2)\n",
        "warped_mask = warp_im(mask, M, im1.shape)\n",
        "combined_mask = numpy.max([get_face_mask(im1, landmarks1), warped_mask],\n",
        "                          axis=0)\n",
        "\n",
        "warped_im2 = warp_im(im2, M, im1.shape)\n",
        "warped_corrected_im2 = correct_colours(im1, warped_im2, landmarks1)\n",
        "\n",
        "output_im = im1 * (1.0 - combined_mask) + warped_corrected_im2 * combined_mask\n",
        "\n",
        "cv2.imwrite('./resultado.jpg', output_im)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVYdQ83Gl63O",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# EXERCICIO 2 - Troca de rosto em video\n",
        "Substitua os nomes \"VIDEO1\" e \"VIDEO2\" pelo nome dos videos que você deseja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeFYftDkRDka",
        "colab_type": "code",
        "outputId": "297c5606-e79a-4731-81b7-20417c740a13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!/usr/bin/python\n",
        "\n",
        "\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "import sys\n",
        "VIDEO1='download.mp4'\n",
        "VIDEO2='pica.mp4'\n",
        "TEMPO=10\n",
        "\n",
        "\n",
        "try:\n",
        "    os.system('mkdir corpo')\n",
        "    os.system('mkdir res')\n",
        "    os.system('mkdir face')\n",
        "    pass\n",
        "except expression as identifier:\n",
        "    pass\n",
        "os.system('ffmpeg -i   %s   -vf fps=25  -t %s      ./corpo/%s.jpeg' %(VIDEO1,TEMPO,'%d'))\n",
        "os.system('ffmpeg -i   %s   -vf fps=25  -t %s      ./face/%s.jpeg' %(VIDEO2,TEMPO,'%d'))\n",
        "\n",
        "PREDICTOR_PATH = \"shape_predictor_68_face_landmarks.dat?raw=true\"\n",
        "SCALE_FACTOR = 1\n",
        "FEATHER_AMOUNT = 11\n",
        "\n",
        "FACE_POINTS = list(range(17, 68))\n",
        "MOUTH_POINTS = list(range(48, 61))\n",
        "RIGHT_BROW_POINTS = list(range(17, 22))\n",
        "LEFT_BROW_POINTS = list(range(22, 27))\n",
        "RIGHT_EYE_POINTS = list(range(36, 42))\n",
        "LEFT_EYE_POINTS = list(range(42, 48))\n",
        "NOSE_POINTS = list(range(27, 35))\n",
        "JAW_POINTS = list(range(0, 17))\n",
        "\n",
        "ALIGN_POINTS = (LEFT_BROW_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS +\n",
        "                               RIGHT_BROW_POINTS + NOSE_POINTS + MOUTH_POINTS)\n",
        "\n",
        "\n",
        "OVERLAY_POINTS = [\n",
        "    LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,\n",
        "    NOSE_POINTS + MOUTH_POINTS\n",
        "]\n",
        "\n",
        "COLOUR_CORRECT_BLUR_FRAC = .6\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(PREDICTOR_PATH)\n",
        "\n",
        "class TooManyFaces(Exception):\n",
        "    pass\n",
        "\n",
        "class NoFaces(Exception):\n",
        "    pass\n",
        "\n",
        "def get_landmarks(im):\n",
        "    rects = detector(im, 1)\n",
        "    \n",
        "    if len(rects) > 1:\n",
        "        raise TooManyFaces\n",
        "    if len(rects) == 0:\n",
        "        raise NoFaces\n",
        "\n",
        "    return numpy.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])\n",
        "\n",
        "def annotate_landmarks(im, landmarks):\n",
        "    im = im.copy()\n",
        "    for idx, point in enumerate(landmarks):\n",
        "        pos = (point[0, 0], point[0, 1])\n",
        "        cv2.putText(im, str(idx), pos,\n",
        "                    fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n",
        "                    fontScale=0.4,\n",
        "                    color=(0, 0, 255))\n",
        "        cv2.circle(im, pos, 3, color=(0, 255, 255))\n",
        "    return im\n",
        "\n",
        "def draw_convex_hull(im, points, color):\n",
        "    points = cv2.convexHull(points)\n",
        "    cv2.fillConvexPoly(im, points, color=color)\n",
        "\n",
        "def get_face_mask(im, landmarks):\n",
        "    im = numpy.zeros(im.shape[:2], dtype=numpy.float64)\n",
        "\n",
        "    for group in OVERLAY_POINTS:\n",
        "        draw_convex_hull(im,\n",
        "                         landmarks[group],\n",
        "                         color=1)\n",
        "\n",
        "    im = numpy.array([im, im, im]).transpose((1, 2, 0))\n",
        "\n",
        "    im = (cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0) > 0) * 1.0\n",
        "    im = cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0)\n",
        "\n",
        "    return im\n",
        "    \n",
        "def transformation_from_points(points1, points2):\n",
        "    \"\"\"\n",
        "    Return an affine transformation [s * R | T] such that:\n",
        "\n",
        "        sum ||s*R*p1,i + T - p2,i||^2\n",
        "\n",
        "    is minimized.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    points1 = points1.astype(numpy.float64)\n",
        "    points2 = points2.astype(numpy.float64)\n",
        "\n",
        "    c1 = numpy.mean(points1, axis=0)\n",
        "    c2 = numpy.mean(points2, axis=0)\n",
        "    points1 -= c1\n",
        "    points2 -= c2\n",
        "\n",
        "    s1 = numpy.std(points1)\n",
        "    s2 = numpy.std(points2)\n",
        "    points1 /= s1\n",
        "    points2 /= s2\n",
        "\n",
        "    U, S, Vt = numpy.linalg.svd(points1.T * points2)\n",
        "\n",
        "    R = (U * Vt).T\n",
        "\n",
        "    return numpy.vstack([numpy.hstack(((s2 / s1) * R,\n",
        "                                       c2.T - (s2 / s1) * R * c1.T)),\n",
        "                         numpy.matrix([0., 0., 1.])])\n",
        "\n",
        "def read_im_and_landmarks(fname):\n",
        "    im = cv2.imread(fname, cv2.IMREAD_COLOR)\n",
        "    im = cv2.resize(im, (im.shape[1] * SCALE_FACTOR,\n",
        "                         im.shape[0] * SCALE_FACTOR))\n",
        "    s = get_landmarks(im)\n",
        "\n",
        "    return im, s\n",
        "\n",
        "def warp_im(im, M, dshape):\n",
        "    output_im = numpy.zeros(dshape, dtype=im.dtype)\n",
        "    cv2.warpAffine(im,\n",
        "                   M[:2],\n",
        "                   (dshape[1], dshape[0]),\n",
        "                   dst=output_im,\n",
        "                   borderMode=cv2.BORDER_TRANSPARENT,\n",
        "                   flags=cv2.WARP_INVERSE_MAP)\n",
        "    return output_im\n",
        "\n",
        "def correct_colours(im1, im2, landmarks1):\n",
        "    blur_amount = COLOUR_CORRECT_BLUR_FRAC * numpy.linalg.norm(\n",
        "                              numpy.mean(landmarks1[LEFT_EYE_POINTS], axis=0) -\n",
        "                              numpy.mean(landmarks1[RIGHT_EYE_POINTS], axis=0))\n",
        "    blur_amount = int(blur_amount)\n",
        "    if blur_amount % 2 == 0:\n",
        "        blur_amount += 1\n",
        "    im1_blur = cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)\n",
        "    im2_blur = cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)\n",
        "\n",
        "    # Avoid divide-by-zero errors.\n",
        "    im2_blur += (128 * (im2_blur <= 1.0)).astype(im2_blur.dtype)\n",
        "\n",
        "    return (im2.astype(numpy.float64) * im1_blur.astype(numpy.float64) /\n",
        "                                                im2_blur.astype(numpy.float64))\n",
        "\n",
        "def run1(f1,f2,f3):\n",
        "    im1, landmarks1 = read_im_and_landmarks(f1)\n",
        "    im2, landmarks2 = read_im_and_landmarks(f2)\n",
        "\n",
        "    M = transformation_from_points(landmarks1[ALIGN_POINTS],\n",
        "                                landmarks2[ALIGN_POINTS])\n",
        "\n",
        "    mask = get_face_mask(im2, landmarks2)\n",
        "    warped_mask = warp_im(mask, M, im1.shape)\n",
        "    combined_mask = numpy.max([get_face_mask(im1, landmarks1), warped_mask],\n",
        "                            axis=0)\n",
        "\n",
        "    warped_im2 = warp_im(im2, M, im1.shape)\n",
        "    warped_corrected_im2 = correct_colours(im1, warped_im2, landmarks1)\n",
        "\n",
        "    output_im = im1 * (1.0 - combined_mask) + warped_corrected_im2 * combined_mask\n",
        "\n",
        "    cv2.imwrite(f3, output_im)\n",
        "\n",
        "a=range(25*TEMPO)\n",
        "worked=1\n",
        "for x in a:\n",
        "    print(x)\n",
        "    try:\n",
        "        run1('./corpo/%s.jpeg' % x,'./face/%s.jpeg' % x,'./res/%s.jpeg' % x )\n",
        "        worked=x\n",
        "    except:\n",
        "\n",
        "        run1('./corpo/%s.jpeg' % worked,'./face/%s.jpeg' % worked,'./res/%s.jpeg' % x )\n",
        "        pass\n",
        "    \n",
        "os.system('ffmpeg -y -r 25 -i res/%s.jpeg     -i %s -acodec copy -c:v copy -c:a aac -map 0:v:0 -map 1:a:0  -t %s   out.mp4' %('%d',VIDEO2,TEMPO))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    }
  ]
}